{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **K-Means**\n",
    "\n",
    "**1- Optimization Objective:**\n",
    "\n",
    "- Distortion Measure J:\n",
    "\n",
    "    - K-Means aims to partition data into K clusters by minimizng the within-cluster sum of squares (WCSS), also known as the distortion measure J.\n",
    "\n",
    "    - The objective function represents the total variance within clusters, and minimizing it leads to compact and well separated clusters.\n",
    "\n",
    "- Piecewise Optimization:\n",
    "\n",
    "    - The algorithm performs an iterative optimization, alternatively minimizing J with respect to cluster assignments $r_{nk}$ and centroids $u_{k0}$.\n",
    "\n",
    "    - This approach is an instance of coordinate descent, optimizing one set of variables while keeping others fixed.\n",
    "\n",
    "**2- Lloyd's Algorithm**\n",
    "\n",
    "- Historical Context:\n",
    "\n",
    "    - K-Means is ofted associated with Lloy'd algorithm, introduced by Stuart Lloyd in 1957 for signal processing applications.\n",
    "\n",
    "    - The algorithm's iterative refinement approach is fundamental to its theorical framework.\n",
    "\n",
    "- Convergence Properties:\n",
    "\n",
    "    - Monotonic Decrease of J:\n",
    "\n",
    "        - Each iteration of K-Means decreases the distortion measure J or leaves it unchanged.\n",
    "\n",
    "        - This is due to the assignment and update steps each minimizing J with respect to $r_{nk}$ and $u_{k0}$ respectively.\n",
    "\n",
    "    - Convergence to Local Minimum:\n",
    "\n",
    "        - K-Means is guaranted to converge to a local minimum of J after a finite number of iterations.\n",
    "\n",
    "        - However, it does not guarantee convergence to the global minimum due to its sensitivity to initial centroid positions.\n",
    "\n",
    "**3- Relation to EM algorithm**\n",
    "\n",
    "- EM algorithm Overview:\n",
    "\n",
    "    - The EM algorithm is a general approach for finding maximum likelihood estimates in probabilistic models with latent variables.\n",
    "\n",
    "    - It alternates between estimating the expected value of the latent variables (E-step) and maximizing the likelihood function (M-step).\n",
    "\n",
    "- K-Means as a special case:\n",
    "\n",
    "    - K-Means can be viewed as a hard assignment verson of the EM algorithm applied to GMMs with equal covariance matrices and zero covariances (identity matrices scaled by a constant).\n",
    "\n",
    "    - In this context:\n",
    "\n",
    "        - E-Step: Assigns each data point to the nearest cluster (hard assignments).\n",
    "\n",
    "        - M-Step: Updates cluster centroids based on current assignments.\n",
    "\n",
    "**4- Assumptions Underlying K-Means.**\n",
    "\n",
    "- Euclidean Distance Metric:\n",
    "\n",
    "    - K-Means assumes that the Euclidean distance is a meaningful measure of similarity between data points.\n",
    "\n",
    "    - This implies that clusters are spherical in shape and separable in Euclidean space.\n",
    "\n",
    "- Equal variance and Isotropy:\n",
    "\n",
    "    - Implicitly assumes that all clusters have the same variance and are isotropic (uniform in all directions).\n",
    "\n",
    "    - Clusters are expected to be roughly similinar in size and density.\n",
    "\n",
    "- Feature Independence:\n",
    "\n",
    "    - Assumes that features contribute equally and independently to the distance calculations.\n",
    "\n",
    "    - This underscores the importance of feature scaling and preprocessing.\n",
    "\n",
    "**5- Theorical Limitations**\n",
    "\n",
    "- Sensitivity to Initialization:\n",
    "\n",
    "    - Different initializations can lead to different clustering results due to convergence to local minima.\n",
    "\n",
    "    - Theoretical implications include the non-convexity of the objective function J.\n",
    "\n",
    "- Cluster Shape Constraints:\n",
    "\n",
    "    - K-Means performs poorly on data with non-spherical clusters, varying sizes, or densities.\n",
    "\n",
    "    - The alogrithm theoretical framwork is not designed to handle such complexities.\n",
    "\n",
    "- Outlier influence:\n",
    "\n",
    "    - Outliers can disproportionately affect centroid positions due to the use of the mean in centroid calculation.\n",
    "\n",
    "    - Theoretically, this is because the mean is sensitive to extreme values.\n",
    "\n",
    "**6- Computational Complexity**\n",
    "\n",
    "- Time complexity:\n",
    "\n",
    "    - Per iteration complexity: O(NKD), where:\n",
    "        \n",
    "        - N: Number of data points.\n",
    "        - K: Number of clusters.\n",
    "        - D: Dimensionality of the data.\n",
    "\n",
    "    - Total Complexity: Dependes on the number of iterations until convergence, which is theretically finite.\n",
    "\n",
    "- Scalability:\n",
    "\n",
    "    - K-Means is efficient for large datasets due to its linear complexity with respect to N and D.\n",
    "\n",
    "**7- CHossing the number of clusters K**\n",
    "\n",
    "- No theoretical Determination:\n",
    "\n",
    "    - The algorithm does not inherently determine the optimal K.\n",
    "\n",
    "    - Theroetically, selecting K is a model selection problem.\n",
    "\n",
    "- Model Selection criteria:\n",
    "\n",
    "    - Elbow method: Looks for a point where adding another cluster doesn't significantly reduce J.\n",
    "\n",
    "    - Information Criteria: Akaike information criterion (AIC) or bayesian information Criterion (BIC) can be used in probabilistic models related to K-Means.\n",
    "\n",
    "    - Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "\n",
    "**8- Algotihmic Properties**\n",
    "\n",
    "- Non-Convex Optimization\n",
    "\n",
    "    - Objective Function Landsacpe:\n",
    "\n",
    "        - The distortion measure J is non-convex with respect to $r_{nk}$ and $u_{k}$ jointly.\n",
    "\n",
    "        - Theretical implication is that multiple local minima exist, and global optimization is computationally infeasible.\n",
    "\n",
    "- Stability and Consistency\n",
    "\n",
    "    - Algorithm Stability:\n",
    "\n",
    "        - The clustering results can vary with different runs due to random initialization.\n",
    "\n",
    "        - Theoretically, algorithms with higher stability are preffered for consistent results.\n",
    "\n",
    "    - Statistical Consistency:\n",
    "\n",
    "        - Under certain conditions (e.g., large sample sizes, appropiate K), K-Means can consistetnly recover true cluster structures.\n",
    "\n",
    "**9- Practical Theoretical Considerations**\n",
    "\n",
    "- Feature Engineering\n",
    "\n",
    "    - Impact on distance calculations:\n",
    "\n",
    "        - The choice and transformation of features affect the theoretical validity of the distance metric.\n",
    "\n",
    "        - Theoretical considereation of feature importance and correlation is crucial.\n",
    "\n",
    "- High Dimension data\n",
    "\n",
    "    - Curse of Dimensionality:\n",
    "\n",
    "        - In high-dimensional spaces, distances between points become less meaninful.\n",
    "\n",
    "        - Theoretically, this affects the reliability of K-Means, necessitating dimensionality reduction techniques.\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
