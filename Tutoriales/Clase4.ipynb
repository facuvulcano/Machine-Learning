{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Tutorial 4: Regresión Lineal Localmente Ponderada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando los datos\n",
    "\n",
    "- Supongamos que tenemos el problema supervisado de predecir precios de casas:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/house_prices.jpg\">\n",
    "</div>\n",
    "\n",
    "- Una pregunta que debemos hacernos cuando buscamos ajustar un modelo a los datos es: <span style=\"color: blue;\">_¿Cuáles son los features que necesito/quiero?_</span>\n",
    "- Podriamos ajustar usando...\n",
    "    - Una recta de la forma: $w_0 + w_1 x_1$\n",
    "    - Una cuadrática de la forma: $w_0 + w_1 x_1 + w_2 x_2^2 $\n",
    "    - Un ajuste personalizado como: $w_0 + w_1 x + w_2 \\sqrt x + w_3 log(x) $\n",
    "        - En este caso, deberiamos definir nuestras features como: \n",
    "            - $x_1 = x$\n",
    "            - $x_2 = \\sqrt x$\n",
    "            - $x_3 = log(x)$\n",
    "\n",
    "- Al definir estas nuevas características, la maquinaria de regresión lineal se presta naturalmente para ajustarse a estas funciones de features de entrada en su conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de selección de características\n",
    "- Son algoritmos para decidir de forma automática cual es el conjunto de features que mejor ajusta a nuestros datos.\n",
    "- Permiten responder la pregunta: <span style=\"color: blue;\">_¿Cuál es la combinación de __funciones de nuestras features de entrada__ que resultan en la mejor performance del modelo?_</span>\n",
    "    - En otras palabras, ¿qué funciones de x (por ejemplo, $x^2$, $\\sqrt x$, $log(x)$, $x^3$, $x^{2/3}$) resultan como features apropiadas para usar? \n",
    "- Para ajustar a un dataset que es inherentemente no lineal, una línea recta no sería la mejor opción y tendríamos que explorar polinomios.\n",
    "- Para abordar el problema de ajustar datos que no son lineales y descubrir la mejor combinación de funciones de sus características a utilizar, entra en juego la regresión localmente ponderada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal Localmente Ponderada\n",
    "- Es un algoritmo que modifica la regresión lineal para adaptarla a funciones no lineales.\n",
    "- Consideremos el problema de predecir $y$ a partir de los valores de $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_values = [1, 2, 3, 4, 5, 6]\n",
    "y_values = [1, 2.12, 3.1, 3.5, 3.58, 3.8]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b')\n",
    "plt.title('Plot of x vs y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1️⃣ Si ajustamos $y = w_0 + w_1 x$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos y = w_0 + w_1*x\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n",
    "W = np.linalg.inv(X.T @ X) @ X.T @ Y  # Calculate the parameters w_0 and w_1\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = W[0] + W[1] * x_fit\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x')\n",
    "\n",
    "plt.title('Plot of x vs y with Linear Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ➡️ Vemos que los datos no están realmente en línea recta, por lo que el ajuste no es muy bueno. \n",
    ">\n",
    "> <font color=red>__Underfitting.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2️⃣ Si ajustamos:  $y = w_0 + w_1 x + w_2 x^2$ (agregamos una feature más: $x^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos cuadratica y = w_0 + w_1*x + w_2*x^2\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X, X**2))  # Agregamos columnas para x and x^2\n",
    "W = np.linalg.inv(X.T @ X) @ X.T @ Y  # Calculamos los parameters w_0, w_1, and w_2\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = W[0] + W[1] * x_fit + W[2] * x_fit**2\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x + {W[2]:.2f}x^2')\n",
    "\n",
    "plt.title('Plot of x vs y with Quadratic Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ➡️ Vemos que obtenemos un mejor ajuste a los datos.\n",
    ">\n",
    "> <font color=green>__Good fit.__</font>\n",
    "\n",
    "- Podría parecer que cuantas más features agregamos, mejor. Pero ya vimos en clases anteriores que hay cierto riesgo en agregar demasiadas features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3️⃣ Si ajustamos $y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + w_5 x^5$ (polinomio de orden 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos polinomio de orden 5 y = w_0 + w_1*x + ... + w_5*x^5\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X_poly = np.hstack([X**i for i in range(6)])  # Agregamos columnas para x^0, x^1, x^2, ..., x^5\n",
    "W = np.linalg.inv(X_poly.T @ X_poly) @ X_poly.T @ Y  # Calculamos parametros w_0, w_1, ..., w_5\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = sum(W[i] * x_fit**i for i in range(6))\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x + {W[2]:.2f}x^2 + {W[3]:.2f}x^3 + {W[4]:.2f}x^4 + {W[5]:.2f}x^5')\n",
    "\n",
    "plt.title('Plot of x vs y with Polynomial Fit (Order 5)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ➡️ Vemos que aunque la curva ajustada pasa perfectamente los datos, no esperamos que sea un buen predictor de precios de casas. \n",
    "> \n",
    "> <font color=red>__Overfitting.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como vemos en las gráficas, <font color=blue>la elección de features es crucial para asegurar una buen performance</font>.\n",
    "- Veamos en detalle la regresión lineal localmente ponderada, la cual (asumiendo que hay suficiente data de entrenamiento), hace que la elección de features sea menos crítica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procedimiento: Regresión Lineal Localmente Ponderada\n",
    "- Ajustamos una función lineal solamente a un conjunto local de puntos delimitados por una región, utilizando mínimos cuadrados ponderados.\n",
    "- Los pesos están dados por las alturas de una __función de ponderación (kernel function)__ que nos da:\n",
    "    - Más peso en puntos cercanos al punto objetivo $x_0$ cuya respuesta se está estimando.\n",
    "    - Menos peso a los puntos más lejanos. \n",
    "- Obtenemos entonces un modelo ajustado que retiene solamente los puntos del modelo que están cerca del punto objetivo ($x_0$).\n",
    "- Luego, el punto objetivo se aleja en el eje x y el procedimiento se repite para cada punto.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/lineal4.png\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/loess.gif\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos evaluar la función de hipótesis $h$ en cierto punto query $x_0$. \n",
    "- En regresión lineal:\n",
    "    - Ajustamos $w$ para minimizar la función de costo $$J(W) =  \\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i}(x_i, w))^{2} $$\n",
    "    - Obtenemos como salida $w^Tx$\n",
    "\n",
    "- En regresión lineal __localmente ponderada__:\n",
    "    - Ajustamos $w$ para minimizar la función de costo $$\\boxed{J(W) =  \\sum_{i=1}^{N}k(x_i,x_0)(y_{i}-\\hat{y}_{i}(x_i, w))^{2}} $$\n",
    "    - Donde $k(x_i,x_0)$ es una __función de ponderación__ que da un valor $∈ [0,1]$ que nos dice que tanto debemos pesar los valores de $(x_i, y_i)$ cuando ajustamos una recta en el vecindario de $x$. En general, una opción común de $k(x_i,x_0)$ es: $$ k(x_i,x_0) = exp (-\\frac{(x_i - x_0)^2}{2\\tau^2})$$\n",
    "    Si $x$ es un vector, entonces se generaliza a:$$ k(x_i,x_0) = exp (-\\frac{(x_i - x_0)^T(x_i - x_0)}{2\\tau^2})$$\n",
    "    De esta forma:\n",
    "    \n",
    "        - Si $|x_i - x_0|$ es chico (el ejemplo $x_i$ esta cerca al punto query $x_0$) → $k(x_i,x_0) \\approx e^0 = 1$.\n",
    "        - Si $|x_i - x_0|$ es grande (el ejemplo $x_i$ esta lejos al punto query $x_0$) → $k(x_i,x_0) = e^{-\\text{num grande}} \\approx 0$.\n",
    "\n",
    "        <span style=\"color: blue;\">Geometricamente</span>, el peso de las muestras individuales en el vecindario de $x_0$ son representadas por la <span style=\"color: blue;\">altura de la curva $k(x_i,x_0)$ en ese punto</span>. Como la curva esta centrada en $x_0$, se pesan más las muestras más cercanas a el.\n",
    "\n",
    "        _Nota: Si bien es una curva con forma de campana, esto NO es una función de densidad de probabilidad Gaussiana. Los $k(x_i,x_0)$ no tienen que ver con gaussianas, y en particular no son variables aleatorias normalmente distribuidas. Además, $k(x_i,x_0)$ no integra a 1._\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parámetro de Ancho de Banda\n",
    "- El hiperparámetro $\\tau$ decide el ancho de la vecindad que se debe mirar para ajustar la regresión lineal localmente ponderada.\n",
    "- Mide que tan rápido el peso de un ejemplo de entrenamiento cae con la distancia desde el punto de query.\n",
    "    - Si $\\tau$ es muy ancho: underfitting.\n",
    "    - Si $\\tau$ es muy delgado: overfitting.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 30%;\" src=\"img/weight_function.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizaje Paramétrico vs. No Paramétrico\n",
    "- Regresión Lineal\n",
    "    - La regresión lineal (no ponderada) es un __algoritmo de aprendizaje paramétrico__, es decir, <span style=\"color: blue;\">ajusta un conjunto fijo de parámetros</span> $w$ a tus datos.\n",
    "    - No importa cuán grande sea tu conjunto de entrenamiento, una vez que entrenas tu modelo y ajustas los parámetros, podrías borrar todo el conjunto de entrenamiento de la memoria y hacer predicciones solo usando los parámetros $w$. \n",
    "        - Tus parámetros representan así un resumen de tu conjunto de datos original.\n",
    "\n",
    "- Regresión Lineal Localmente Ponderada\n",
    "    - La regresión ponderada localmente es un __algoritmo de aprendizaje no paramétrico__. \n",
    "    - La cantidad de datos que necesitas mantener para representar la hipótesis $h(\\cdot)$ crece con el tamaño del conjunto de entrenamiento. \n",
    "        - En el caso de la regresión localmente ponderada, esta tasa de crecimiento es lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ventajas y desventajas de la Regresión Localmente Ponderada\n",
    "\n",
    "#### 👍 <font color=green>Ventajas</font>\n",
    "- Principal ventaja: ajusta a un dataset no lineal sin necesidad de manipular manualmente las features (¡y evitamos overfitting!).\n",
    "- No requiere la especificación de una función para ajustar un modelo a todos los datos de la muestra.\n",
    "- Solo requiere como parámetros la función de ponderación y $\\tau$.\n",
    "- Es muy flexible,  puede modelar procesos complejos para los que no existe ningún modelo teórico.\n",
    "- Es considerado uno de los métodos de regresión modernos más atractivos para aplicaciones que se ajustan al marco general de la regresión de mínimos cuadrados pero que tienen una estructura determinista compleja.\n",
    "\n",
    "#### 👎 <font color=red>Desventajas</font>\n",
    "- Requiere mantener todo el conjunto de entrenamiento para poder hacer predicciones futuras.\n",
    "- El número de parámetros crece linealmente con el tamaño del conjunto de entrenamiento.\n",
    "- Computacionalmente intensivo, ya que se calcula un modelo de regresión para cada punto.\n",
    "- Requiere conjuntos de datos bastante grandes y densamente muestreados para producir buenos modelos.\n",
    "- Al igual que otros métodos de mínimos cuadrados, es propensos al efecto de outliers en el conjunto de datos.\n",
    "- Si bien se puede generalizar a mas dimensiones, si es mayor a 3 o 4 dimensiones puede no performar bien porque en general no van a haber muchas muestras cercanas al punto de query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Implementación en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(2 * np.pi * x)\n",
    "y_noise = y + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x,y,color = 'darkblue', label = 'f(x)')\n",
    "plt.scatter(x, y_noise, facecolors = 'none', edgecolor = 'darkblue', label = 'f(x) + noise')\n",
    "plt.fill(x[:40],np.exp(-(x[:40] - 0.2)**2/(2*((0.05)**2))), color = 'pink', alpha = .5, label = 'Kernel')\n",
    "plt.legend()\n",
    "plt.title(\"Función senoidal a aproximar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución en Forma Cerrada para Regresión Lineal Localmente Ponderada\n",
    "\n",
    "- Para llegar a la expresión dada para los parámetros $ W $, necesitamos minimizar $ J(W) $. \n",
    "\n",
    "- Para eso, debemos calcular su gradiente con respecto a $ W $ e igualar igual a cero, ya que estamos buscando el punto donde la función de costo alcanza su mínimo. \n",
    "\n",
    "- Sustituyendo $ \\hat{y}_{i}(x_i, w) = x_i^T w $ en $ J(W) $, obtenemos:\n",
    "\n",
    "$$\n",
    "J(W) = \\sum_{i=1}^{N} k(x_i,x_0)(y_{i}-x_i^T w)^2\n",
    "$$\n",
    "\n",
    "- Podemos definir el problema de forma matricial, donde:\n",
    "\n",
    "    - $ X $ es la matriz de diseño que contiene los vectores de características $ x_i^T$ como filas. \n",
    "    - $ y $ es el vector de valores objetivo.\n",
    "    - $ K(X_0) $ es la matriz diagonal de ponderación, donde cada elemento de la diagonal $ k_{ii} = k(x_i,x_0) $ representa el peso de la observación $x_i$ basado en su distancia al punto de query $x_0$.\n",
    "\n",
    "$$\n",
    "J(W) = (y - XW)^T K(X_0) (y - XW)\n",
    "$$\n",
    "\n",
    "- Si expandimos la expresión del error cuadrático ponderado:\n",
    "\n",
    "$$\n",
    "J(W) = y^T K(x_0) y - y^T K(x_0) XW - (XW)^T K(x_0) y + (XW)^T K(x_0) XW\n",
    "$$\n",
    "\n",
    "- Tomando su derivada respecto a $W$, obtenemos:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(W)}{\\partial W} = 2 X^T K(x_0) X W - 2 X^T K(x_0) y\n",
    "$$\n",
    "\n",
    "- Finalmente, igualamos a 0:\n",
    "\n",
    "$$\n",
    "\\boxed{W^* = (X^T K(X_0) X)^{-1} (X^T K(X_0) y)}\n",
    "$$\n",
    "\n",
    "Esta es la expresión para los parámetros $ W^* $ utilizando la solución en forma cerrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LWR import LocallyWeightedRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_values = [0.001, 0.1, 1.0]\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(2 * np.pi * x)\n",
    "y_noise = y + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "all_residuals = []\n",
    "for tau in tau_values:\n",
    "    lw_regression = LocallyWeightedRegression(tau)\n",
    "    X = x.reshape(-1, 1) \n",
    "    Y = y_noise.reshape(-1, 1) \n",
    "    query_x = np.linspace(0, 1, 100) # Predict output for a range of query points\n",
    "    predictions = []\n",
    "    for qx in query_x:\n",
    "        _, pred = lw_regression.predict(X, Y, qx)\n",
    "        predictions.append(pred.item())\n",
    "    residuals = np.abs(y_noise - np.array(predictions))\n",
    "    all_residuals.append(residuals)\n",
    "\n",
    "plt.figure(figsize=(20, 15));\n",
    "for i, tau in enumerate(tau_values):\n",
    "    lw_regression = LocallyWeightedRegression(tau)\n",
    "    X = x.reshape(-1, 1) \n",
    "    Y = y_noise.reshape(-1, 1) \n",
    "    query_x = np.linspace(0, 1, 100) # Predict output for a range of query points\n",
    "    predictions = []\n",
    "    for qx in query_x:\n",
    "        _, pred = lw_regression.predict(X, Y, qx)\n",
    "        predictions.append(pred.item())\n",
    "    residuals = np.abs(y_noise - np.array(predictions))\n",
    "    \n",
    "    # Plot scatter plot with locally weighted regression fit\n",
    "    plt.subplot(3, 3, i*3 + 1)\n",
    "    plt.plot(x, y, color='darkblue', label='f(x)')\n",
    "    plt.scatter(x, y_noise, facecolors='none', edgecolor='darkblue', label='f(x) + noise')\n",
    "    plt.plot(query_x, predictions, color='red', label='Locally Weighted Regression')\n",
    "    plt.title(f'$\\\\tau = {tau}$')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot histogram of absolute residuals\n",
    "    plt.subplot(3, 3, i*3 + 2)\n",
    "    plt.hist(residuals, bins=10, color='m', edgecolor='black', range=(0, 1))  \n",
    "    plt.title(f'Residuals Histogram ($\\\\tau = {tau}$)')\n",
    "    plt.xlabel('Absolute Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "    # Scatter plot of Predicted vs. Actual values\n",
    "    plt.subplot(3, 3, i*3 + 3)\n",
    "    plt.scatter(y_noise, np.array(predictions), color='blue', edgecolor='black')\n",
    "    plt.plot([-1, 1], [-1, 1], color='red', linestyle='--') \n",
    "    plt.title('Predicted vs. Actual Values')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
