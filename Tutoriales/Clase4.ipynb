{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Tutorial 4: Regresi√≥n Lineal Localmente Ponderada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando los datos\n",
    "\n",
    "- Supongamos que tenemos el problema supervisado de predecir precios de casas:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/house_prices.jpg\">\n",
    "</div>\n",
    "\n",
    "- Una pregunta que debemos hacernos cuando buscamos ajustar un modelo a los datos es: <span style=\"color: blue;\">_¬øCu√°les son los features que necesito/quiero?_</span>\n",
    "- Podriamos ajustar usando...\n",
    "    - Una recta de la forma: $w_0 + w_1 x_1$\n",
    "    - Una cuadr√°tica de la forma: $w_0 + w_1 x_1 + w_2 x_2^2 $\n",
    "    - Un ajuste personalizado como: $w_0 + w_1 x + w_2 \\sqrt x + w_3 log(x) $\n",
    "        - En este caso, deberiamos definir nuestras features como: \n",
    "            - $x_1 = x$\n",
    "            - $x_2 = \\sqrt x$\n",
    "            - $x_3 = log(x)$\n",
    "\n",
    "- Al definir estas nuevas caracter√≠sticas, la maquinaria de regresi√≥n lineal se presta naturalmente para ajustarse a estas funciones de features de entrada en su conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de selecci√≥n de caracter√≠sticas\n",
    "- Son algoritmos para decidir de forma autom√°tica cual es el conjunto de features que mejor ajusta a nuestros datos.\n",
    "- Permiten responder la pregunta: <span style=\"color: blue;\">_¬øCu√°l es la combinaci√≥n de __funciones de nuestras features de entrada__ que resultan en la mejor performance del modelo?_</span>\n",
    "    - En otras palabras, ¬øqu√© funciones de x (por ejemplo, $x^2$, $\\sqrt x$, $log(x)$, $x^3$, $x^{2/3}$) resultan como features apropiadas para usar? \n",
    "- Para ajustar a un dataset que es inherentemente no lineal, una l√≠nea recta no ser√≠a la mejor opci√≥n y tendr√≠amos que explorar polinomios.\n",
    "- Para abordar el problema de ajustar datos que no son lineales y descubrir la mejor combinaci√≥n de funciones de sus caracter√≠sticas a utilizar, entra en juego la regresi√≥n localmente ponderada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n Lineal Localmente Ponderada\n",
    "- Es un algoritmo que modifica la regresi√≥n lineal para adaptarla a funciones no lineales.\n",
    "- Consideremos el problema de predecir $y$ a partir de los valores de $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_values = [1, 2, 3, 4, 5, 6]\n",
    "y_values = [1, 2.12, 3.1, 3.5, 3.58, 3.8]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b')\n",
    "plt.title('Plot of x vs y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Si ajustamos $y = w_0 + w_1 x$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos y = w_0 + w_1*x\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n",
    "W = np.linalg.inv(X.T @ X) @ X.T @ Y  # Calculate the parameters w_0 and w_1\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = W[0] + W[1] * x_fit\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x')\n",
    "\n",
    "plt.title('Plot of x vs y with Linear Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚û°Ô∏è Vemos que los datos no est√°n realmente en l√≠nea recta, por lo que el ajuste no es muy bueno. \n",
    ">\n",
    "> <font color=red>__Underfitting.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Si ajustamos:  $y = w_0 + w_1 x + w_2 x^2$ (agregamos una feature m√°s: $x^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos cuadratica y = w_0 + w_1*x + w_2*x^2\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X, X**2))  # Agregamos columnas para x and x^2\n",
    "W = np.linalg.inv(X.T @ X) @ X.T @ Y  # Calculamos los parameters w_0, w_1, and w_2\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = W[0] + W[1] * x_fit + W[2] * x_fit**2\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x + {W[2]:.2f}x^2')\n",
    "\n",
    "plt.title('Plot of x vs y with Quadratic Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚û°Ô∏è Vemos que obtenemos un mejor ajuste a los datos.\n",
    ">\n",
    "> <font color=green>__Good fit.__</font>\n",
    "\n",
    "- Podr√≠a parecer que cuantas m√°s features agregamos, mejor. Pero ya vimos en clases anteriores que hay cierto riesgo en agregar demasiadas features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3Ô∏è‚É£ Si ajustamos $y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + w_5 x^5$ (polinomio de orden 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_values, y_values, marker='o', color='b', label='Data')\n",
    "\n",
    "# Ajustamos polinomio de orden 5 y = w_0 + w_1*x + ... + w_5*x^5\n",
    "X = np.array(x_values).reshape(-1, 1)\n",
    "Y = np.array(y_values)\n",
    "X_poly = np.hstack([X**i for i in range(6)])  # Agregamos columnas para x^0, x^1, x^2, ..., x^5\n",
    "W = np.linalg.inv(X_poly.T @ X_poly) @ X_poly.T @ Y  # Calculamos parametros w_0, w_1, ..., w_5\n",
    "x_fit = np.linspace(min(x_values)-2, max(x_values)+2, 100)\n",
    "y_fit = sum(W[i] * x_fit**i for i in range(6))\n",
    "plt.plot(x_fit, y_fit, color='r', label=f'Fit: y = {W[0]:.2f} + {W[1]:.2f}x + {W[2]:.2f}x^2 + {W[3]:.2f}x^3 + {W[4]:.2f}x^4 + {W[5]:.2f}x^5')\n",
    "\n",
    "plt.title('Plot of x vs y with Polynomial Fit (Order 5)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚û°Ô∏è Vemos que aunque la curva ajustada pasa perfectamente los datos, no esperamos que sea un buen predictor de precios de casas. \n",
    "> \n",
    "> <font color=red>__Overfitting.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como vemos en las gr√°ficas, <font color=blue>la elecci√≥n de features es crucial para asegurar una buen performance</font>.\n",
    "- Veamos en detalle la regresi√≥n lineal localmente ponderada, la cual (asumiendo que hay suficiente data de entrenamiento), hace que la elecci√≥n de features sea menos cr√≠tica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procedimiento: Regresi√≥n Lineal Localmente Ponderada\n",
    "- Ajustamos una funci√≥n lineal solamente a un conjunto local de puntos delimitados por una regi√≥n, utilizando m√≠nimos cuadrados ponderados.\n",
    "- Los pesos est√°n dados por las alturas de una __funci√≥n de ponderaci√≥n (kernel function)__ que nos da:\n",
    "    - M√°s peso en puntos cercanos al punto objetivo $x_0$ cuya respuesta se est√° estimando.\n",
    "    - Menos peso a los puntos m√°s lejanos. \n",
    "- Obtenemos entonces un modelo ajustado que retiene solamente los puntos del modelo que est√°n cerca del punto objetivo ($x_0$).\n",
    "- Luego, el punto objetivo se aleja en el eje x y el procedimiento se repite para cada punto.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/lineal4.png\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 40%;\" src=\"img/loess.gif\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos evaluar la funci√≥n de hip√≥tesis $h$ en cierto punto query $x_0$. \n",
    "- En regresi√≥n lineal:\n",
    "    - Ajustamos $w$ para minimizar la funci√≥n de costo $$J(W) =  \\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i}(x_i, w))^{2} $$\n",
    "    - Obtenemos como salida $w^Tx$\n",
    "\n",
    "- En regresi√≥n lineal __localmente ponderada__:\n",
    "    - Ajustamos $w$ para minimizar la funci√≥n de costo $$\\boxed{J(W) =  \\sum_{i=1}^{N}k(x_i,x_0)(y_{i}-\\hat{y}_{i}(x_i, w))^{2}} $$\n",
    "    - Donde $k(x_i,x_0)$ es una __funci√≥n de ponderaci√≥n__ que da un valor $‚àà [0,1]$ que nos dice que tanto debemos pesar los valores de $(x_i, y_i)$ cuando ajustamos una recta en el vecindario de $x$. En general, una opci√≥n com√∫n de $k(x_i,x_0)$ es: $$ k(x_i,x_0) = exp (-\\frac{(x_i - x_0)^2}{2\\tau^2})$$\n",
    "    Si $x$ es un vector, entonces se generaliza a:$$ k(x_i,x_0) = exp (-\\frac{(x_i - x_0)^T(x_i - x_0)}{2\\tau^2})$$\n",
    "    De esta forma:\n",
    "    \n",
    "        - Si $|x_i - x_0|$ es chico (el ejemplo $x_i$ esta cerca al punto query $x_0$) ‚Üí $k(x_i,x_0) \\approx e^0 = 1$.\n",
    "        - Si $|x_i - x_0|$ es grande (el ejemplo $x_i$ esta lejos al punto query $x_0$) ‚Üí $k(x_i,x_0) = e^{-\\text{num grande}} \\approx 0$.\n",
    "\n",
    "        <span style=\"color: blue;\">Geometricamente</span>, el peso de las muestras individuales en el vecindario de $x_0$ son representadas por la <span style=\"color: blue;\">altura de la curva $k(x_i,x_0)$ en ese punto</span>. Como la curva esta centrada en $x_0$, se pesan m√°s las muestras m√°s cercanas a el.\n",
    "\n",
    "        _Nota: Si bien es una curva con forma de campana, esto NO es una funci√≥n de densidad de probabilidad Gaussiana. Los $k(x_i,x_0)$ no tienen que ver con gaussianas, y en particular no son variables aleatorias normalmente distribuidas. Adem√°s, $k(x_i,x_0)$ no integra a 1._\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Par√°metro de Ancho de Banda\n",
    "- El hiperpar√°metro $\\tau$ decide el ancho de la vecindad que se debe mirar para ajustar la regresi√≥n lineal localmente ponderada.\n",
    "- Mide que tan r√°pido el peso de un ejemplo de entrenamiento cae con la distancia desde el punto de query.\n",
    "    - Si $\\tau$ es muy ancho: underfitting.\n",
    "    - Si $\\tau$ es muy delgado: overfitting.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"width: 30%;\" src=\"img/weight_function.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizaje Param√©trico vs. No Param√©trico\n",
    "- Regresi√≥n Lineal\n",
    "    - La regresi√≥n lineal (no ponderada) es un __algoritmo de aprendizaje param√©trico__, es decir, <span style=\"color: blue;\">ajusta un conjunto fijo de par√°metros</span> $w$ a tus datos.\n",
    "    - No importa cu√°n grande sea tu conjunto de entrenamiento, una vez que entrenas tu modelo y ajustas los par√°metros, podr√≠as borrar todo el conjunto de entrenamiento de la memoria y hacer predicciones solo usando los par√°metros $w$. \n",
    "        - Tus par√°metros representan as√≠ un resumen de tu conjunto de datos original.\n",
    "\n",
    "- Regresi√≥n Lineal Localmente Ponderada\n",
    "    - La regresi√≥n ponderada localmente es un __algoritmo de aprendizaje no param√©trico__. \n",
    "    - La cantidad de datos que necesitas mantener para representar la hip√≥tesis $h(\\cdot)$ crece con el tama√±o del conjunto de entrenamiento. \n",
    "        - En el caso de la regresi√≥n localmente ponderada, esta tasa de crecimiento es lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ventajas y desventajas de la Regresi√≥n Localmente Ponderada\n",
    "\n",
    "#### üëç <font color=green>Ventajas</font>\n",
    "- Principal ventaja: ajusta a un dataset no lineal sin necesidad de manipular manualmente las features (¬°y evitamos overfitting!).\n",
    "- No requiere la especificaci√≥n de una funci√≥n para ajustar un modelo a todos los datos de la muestra.\n",
    "- Solo requiere como par√°metros la funci√≥n de ponderaci√≥n y $\\tau$.\n",
    "- Es muy flexible,  puede modelar procesos complejos para los que no existe ning√∫n modelo te√≥rico.\n",
    "- Es considerado uno de los m√©todos de regresi√≥n modernos m√°s atractivos para aplicaciones que se ajustan al marco general de la regresi√≥n de m√≠nimos cuadrados pero que tienen una estructura determinista compleja.\n",
    "\n",
    "#### üëé <font color=red>Desventajas</font>\n",
    "- Requiere mantener todo el conjunto de entrenamiento para poder hacer predicciones futuras.\n",
    "- El n√∫mero de par√°metros crece linealmente con el tama√±o del conjunto de entrenamiento.\n",
    "- Computacionalmente intensivo, ya que se calcula un modelo de regresi√≥n para cada punto.\n",
    "- Requiere conjuntos de datos bastante grandes y densamente muestreados para producir buenos modelos.\n",
    "- Al igual que otros m√©todos de m√≠nimos cuadrados, es propensos al efecto de outliers en el conjunto de datos.\n",
    "- Si bien se puede generalizar a mas dimensiones, si es mayor a 3 o 4 dimensiones puede no performar bien porque en general no van a haber muchas muestras cercanas al punto de query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Implementaci√≥n en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(2 * np.pi * x)\n",
    "y_noise = y + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x,y,color = 'darkblue', label = 'f(x)')\n",
    "plt.scatter(x, y_noise, facecolors = 'none', edgecolor = 'darkblue', label = 'f(x) + noise')\n",
    "plt.fill(x[:40],np.exp(-(x[:40] - 0.2)**2/(2*((0.05)**2))), color = 'pink', alpha = .5, label = 'Kernel')\n",
    "plt.legend()\n",
    "plt.title(\"Funci√≥n senoidal a aproximar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soluci√≥n en Forma Cerrada para Regresi√≥n Lineal Localmente Ponderada\n",
    "\n",
    "- Para llegar a la expresi√≥n dada para los par√°metros $ W $, necesitamos minimizar $ J(W) $. \n",
    "\n",
    "- Para eso, debemos calcular su gradiente con respecto a $ W $ e igualar igual a cero, ya que estamos buscando el punto donde la funci√≥n de costo alcanza su m√≠nimo. \n",
    "\n",
    "- Sustituyendo $ \\hat{y}_{i}(x_i, w) = x_i^T w $ en $ J(W) $, obtenemos:\n",
    "\n",
    "$$\n",
    "J(W) = \\sum_{i=1}^{N} k(x_i,x_0)(y_{i}-x_i^T w)^2\n",
    "$$\n",
    "\n",
    "- Podemos definir el problema de forma matricial, donde:\n",
    "\n",
    "    - $ X $ es la matriz de dise√±o que contiene los vectores de caracter√≠sticas $ x_i^T$ como filas. \n",
    "    - $ y $ es el vector de valores objetivo.\n",
    "    - $ K(X_0) $ es la matriz diagonal de ponderaci√≥n, donde cada elemento de la diagonal $ k_{ii} = k(x_i,x_0) $ representa el peso de la observaci√≥n $x_i$ basado en su distancia al punto de query $x_0$.\n",
    "\n",
    "$$\n",
    "J(W) = (y - XW)^T K(X_0) (y - XW)\n",
    "$$\n",
    "\n",
    "- Si expandimos la expresi√≥n del error cuadr√°tico ponderado:\n",
    "\n",
    "$$\n",
    "J(W) = y^T K(x_0) y - y^T K(x_0) XW - (XW)^T K(x_0) y + (XW)^T K(x_0) XW\n",
    "$$\n",
    "\n",
    "- Tomando su derivada respecto a $W$, obtenemos:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(W)}{\\partial W} = 2 X^T K(x_0) X W - 2 X^T K(x_0) y\n",
    "$$\n",
    "\n",
    "- Finalmente, igualamos a 0:\n",
    "\n",
    "$$\n",
    "\\boxed{W^* = (X^T K(X_0) X)^{-1} (X^T K(X_0) y)}\n",
    "$$\n",
    "\n",
    "Esta es la expresi√≥n para los par√°metros $ W^* $ utilizando la soluci√≥n en forma cerrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LWR import LocallyWeightedRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_values = [0.001, 0.1, 1.0]\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(2 * np.pi * x)\n",
    "y_noise = y + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "all_residuals = []\n",
    "for tau in tau_values:\n",
    "    lw_regression = LocallyWeightedRegression(tau)\n",
    "    X = x.reshape(-1, 1) \n",
    "    Y = y_noise.reshape(-1, 1) \n",
    "    query_x = np.linspace(0, 1, 100) # Predict output for a range of query points\n",
    "    predictions = []\n",
    "    for qx in query_x:\n",
    "        _, pred = lw_regression.predict(X, Y, qx)\n",
    "        predictions.append(pred.item())\n",
    "    residuals = np.abs(y_noise - np.array(predictions))\n",
    "    all_residuals.append(residuals)\n",
    "\n",
    "plt.figure(figsize=(20, 15));\n",
    "for i, tau in enumerate(tau_values):\n",
    "    lw_regression = LocallyWeightedRegression(tau)\n",
    "    X = x.reshape(-1, 1) \n",
    "    Y = y_noise.reshape(-1, 1) \n",
    "    query_x = np.linspace(0, 1, 100) # Predict output for a range of query points\n",
    "    predictions = []\n",
    "    for qx in query_x:\n",
    "        _, pred = lw_regression.predict(X, Y, qx)\n",
    "        predictions.append(pred.item())\n",
    "    residuals = np.abs(y_noise - np.array(predictions))\n",
    "    \n",
    "    # Plot scatter plot with locally weighted regression fit\n",
    "    plt.subplot(3, 3, i*3 + 1)\n",
    "    plt.plot(x, y, color='darkblue', label='f(x)')\n",
    "    plt.scatter(x, y_noise, facecolors='none', edgecolor='darkblue', label='f(x) + noise')\n",
    "    plt.plot(query_x, predictions, color='red', label='Locally Weighted Regression')\n",
    "    plt.title(f'$\\\\tau = {tau}$')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot histogram of absolute residuals\n",
    "    plt.subplot(3, 3, i*3 + 2)\n",
    "    plt.hist(residuals, bins=10, color='m', edgecolor='black', range=(0, 1))  \n",
    "    plt.title(f'Residuals Histogram ($\\\\tau = {tau}$)')\n",
    "    plt.xlabel('Absolute Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "    # Scatter plot of Predicted vs. Actual values\n",
    "    plt.subplot(3, 3, i*3 + 3)\n",
    "    plt.scatter(y_noise, np.array(predictions), color='blue', edgecolor='black')\n",
    "    plt.plot([-1, 1], [-1, 1], color='red', linestyle='--') \n",
    "    plt.title('Predicted vs. Actual Values')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
